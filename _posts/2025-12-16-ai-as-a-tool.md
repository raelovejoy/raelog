---
layout: post
title: "AI as a tool"
summary: AI as 'cognitive scaffolding,' not replacement
date: 2025-12-16
---

AI is a loaded topic right now. I’m aware of that. I still use it, and because I write a lot, I don’t want that to be vague or implied or quietly hidden in the background.

This isn’t a defense of AI, but rather me being explicit about how it fits into my process.

I used to think the most important part of writing was the words themselves. Over time, I realized that what actually matters is communicating a thought clearly and accurately—getting it from my head into someone else's with as little distortion as possible. Words are just the medium. Clarity is the point.

From that perspective, using tools to reduce friction in expression isn't a compromise. It's a practical response to the limits of language and cognition.

AI is a tool. Not an author, not an authority, not a friend, not a stand-in for judgment. More like a whiteboard that talks back, or a place to dump too many thoughts at once and see what survives.

I mostly use it to clarify things that already exist in my head but haven’t landed cleanly yet. To test an idea from a few angles. To reorganize notes that have grown faster than I can hold them. To make writing clearer, more accessible, and less tangled than it would otherwise be.

Clarity is the point. I write constantly, and the goal isn't speed or volume—it's aligning what I mean with what actually arrives.

That’s also where accessibility comes in.

I have ADHD, and executive dysfunction is a real constraint in how I work. Not in a dramatic way—just in the accumulation of friction. Starting is hard. Holding context is hard. Knowing where to begin when everything feels equally urgent is hard.

AI helps at those points. Not by thinking for me, but by lowering the threshold where my brain tends to stall. It helps me externalize thoughts when they’re all competing for attention at once. It helps me regain momentum without burning myself out trying to brute-force my way through overwhelm.

I don’t really get writer’s block. If anything, I have the opposite problem—too many words pressing forward at the same time. This is one way I create enough structure to work with that instead of against it.

I’m also increasingly interested in local AI and self-hosted systems, longer term. Partly for practical reasons—privacy, reliability, understanding what I’m actually relying on—and partly because I’m wary of how centralized and opaque a lot of this infrastructure has become.

I’d rather treat AI as software I work with than a service I defer to. Exploring local models is a way of learning where the limits actually are, and what tradeoffs I’m making, instead of abstracting all of that away behind an API.

Like any tool or technology, AI requires responsibility, understanding, and boundaries. It doesn’t come with those baked in. Using it well means paying attention to where it helps, where it distorts, and where it shouldn’t be used at all.

It also matters who controls these systems. Much of contemporary AI reflects the priorities of corporations and governments, and that influence isn’t subtle. Still, I don’t think that makes the technology itself worthless. Longer term, I’m [interested](https://github.com/dev11systems/liberated-intelligence) in paths that move AI toward something closer to a commons—less centralized, less extractive, and more accountable to the people actually using it.

That’s a larger conversation for another day, but it’s part of how I think about local models, self-hosting, and staying close to the tools I rely on.

None of that changes the basic boundaries.

AI doesn’t replace my thoughts or judgment. It doesn’t get credit for authorship. It doesn’t excuse mistakes or harm. Anything that ends up here is here because I carefully chose it and take responsibility for it.

There are plenty of valid criticisms of AI—about labor, bias, energy use, power, authorship, all of it. Acknowledging those concerns doesn’t require pretending the tools don’t exist, and using the tools doesn’t require pretending the concerns aren’t real. For me, the only workable approach is limited, conscious use paired with transparency.

I imagine a future where creativity is based not only on one’s ability to master tools—AI included—to accomplish specific tasks like communicating ideas, but also on the ability to think, remain curious, and stay critical.

Creativity has never been just about technical skill, even if it sometimes looked that way. Writing, photography, music—each went through a phase where mastery of the medium was the primary gate. But as tools become more accessible, that gate dissolves.

Anyone can write now. Anyone can take a photograph. In the same way that digital cameras and smartphones expanded photography, AI expands writing. What once required more effort is no longer scarce, and when something goes mainstream, the world tends to saturate with sameness.

The playing field levels. What differentiates the work is no longer how difficult it was to produce, but whether there was something worth saying in the first place.

Tools shape thinking, for better and/or for worse. They always have. Language does. Writing does. Software does. I’d rather make that visible than pretend it isn’t happening.

If something I write is unclear, wrong, or harmful, that’s on me. The presence of AI doesn’t change that.

This is how I’m using it right now. That may change. If it does, I’ll say so.
